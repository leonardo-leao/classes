\documentclass{article}
\usepackage{tpack}

\title{ES572 - Circuitos Lógicos}
\author{Guilherme Nunes Trofino\\217276}
\project{Resumo Teórico}

\begin{document}
    \maketitle
\newpage

    \tableofcontents
\newpage

    \section{Introdução}
        \paragraph{Apresentação}Teoria utilizada para o desenvolvimento da disciplina será baseada nas proposições trazidas por Claude Shannon em 1948.
\begin{multicols}{2}
    \raggedcolumns

    \subsection{Informação}
        \paragraph{Definição}Informação são dados comunicados ou recebidos que resolvem incertezas sobre um fato ou circunstância específica. Assim, dada uma variável aleatória discreta $x$ com as seguintes condições:
            \begin{enumerate}[noitemsep]
                \item Possíveis Valores: $x \in \{x_{1},...,x_{n}\}$;
                \item Probabilidades Associadas: $\{p_{1},...,p_{n}\}$;
            \end{enumerate}
        Desta forma, considera-se $I(x_{i})$ que a \textbf{Quantidade de Informação Recebida}, medida em \texttt{bits}, será relacionada por:
            \begin{equation}
                \boxed{
                    I(x_{i}) = \log_{2}\left(\frac{1}{p_{i}}\right)
                }
            \end{equation}
        Nota-se trata-se de uma informação relacionada apenas ao evento analisado. Além disso, eventos de baixa probabilidade transportam mais informação.

    \columnbreak

    \subsection{Entropia}
        \paragraph{Definição}Dada uma variável aleatória $x$ então sua \textbf{Entropia} $H(x)$ será a quantidade média de informação recebida ao conhecer seu valor, sendo descrita pela equação abaixo:
            \begin{equation*}
                H(x) = E(I(x)) = \sum_{i=1}^{N}p_{i}\log_{2}\left(\frac{1}{p_{i}}\right)
            \end{equation*}
        Onde $E(x)$ representa a \textbf{Esperança} da variável $x$, podendo ser simplificada para:
            \begin{equation}
                \boxed{
                    H(x) = -\sum_{i=1}^{N}p_{i}\log_{2}p_{i}
                }
            \end{equation}
        Nota-se que trata-se de uma informação relacionada apenas ao processo analisado:
            \begin{enumerate}[noitemsep]
                \item Quanto mais baixa, mais previsível; 
                \item Quanto mais alta, mais imprevisível; 
            \end{enumerate}
\end{multicols}
        
        \subsection{Codificação}
            \paragraph{Definição}Mapeamento \textbf{biunívoco}, cada elemento associado a um único contraelemento, entre cadeias de bits e os membros do conjunto de dados a serem condificados. Classificados em:
                \begin{enumerate}[rightmargin = \leftmargin]
                    \item \textbf{Comprimento Fixo:} Caso todos os símbolos ocorram com a mesma probabilidade, geralmente utiliza-se este método;
                        \begin{enumerate}[noitemsep, rightmargin = \leftmargin]
                            \item \texttt{Vantagens:}
                                \begin{enumerate}[noitemsep, rightmargin = \leftmargin]
                                    \item Todas as folhas possuem a mesma distância da raiz;
                                    \item Acesso Aleatório: Variáveis podem ser lidas em qualquer trecho da codificação;
                                \end{enumerate}

                            \item \texttt{Entropia:} Considera-se uma variável aleatória X que assume valores entre $N$ possibilidades equiprováveis será:
                                \begin{equation}
                                    \boxed{
                                        H(x) = \sum_{i=1}^{N}p_{i}\log_{2}\left(\frac{1}{p_{i}}\right) = \sum_{i=1}^{N}\frac{1}{N}\log_{2}(N)
                                    }
                                \end{equation}
                            Desta forma, uma codificação \textbf{ótima} terá $N = 2^{k}$, onde $k \in \mathbb{N}$.
                        \end{enumerate}

                    \item \textbf{Comprimento Variável:} Caso todos os símbolos não ocorram com a mesma probabilidade, geralmente utiliza-se este método;
                        \begin{enumerate}[noitemsep, rightmargin = \leftmargin]
                            \item \texttt{Vantagens:}
                                \begin{enumerate}[noitemsep, rightmargin = \leftmargin]
                                    \item Flexibilidade para se aproximar da codificação \textbf{ideal};
                                    \item Necessária para compresão de arquivos, como descrito por \textbf{Huffman};
                                \end{enumerate}

                            \item \texttt{Entropia:} Considera-se uma variável aleatória X que assume valores entre $N$ possibilidades equiprováveis será:
                                \begin{equation}
                                    \boxed{
                                        H(x) = \sum_{i=1}^{N}p_{i}\log_{2}\left(\frac{1}{p_{i}}\right)
                                    }
                                \end{equation}
                            Desta forma, uma codificação \textbf{ótima} terá:
                                \begin{enumerate}[noitemsep, rightmargin = \leftmargin]
                                    \item \texttt{Codificação Curta:} Se $x_{i}$ tiver uma probabilidade alta;
                                    \item \texttt{Codificação Longa:} Se $x_{i}$ tiver uma probabilidade baixa;
                                \end{enumerate}
                        \end{enumerate}

                    \item \textbf{Codificação Ambígua:} Organização não única dos caracteres envolvidos o que pode gerar problemas de interpretação dos dados. Deve ser \textbf{evitada};

                \end{enumerate}
            Será necessário evitar codificações ambíguas, pois poderá haver incerteza de informação neste caso. Desta forma, uma árvore binária deve ser criada para validar se a codificação é válida, alocando as variáveis nos terminais das ramificações.

        \subsection{Algoritmo de Huffman}
            \paragraph{Definição}Algoritmo para construção de uma \textbf{Árvore Binária Ótima}, isto é uma codificação que possua entropia próxima a mínima necessária. Aplica-se os seguintes passos:
                \begin{enumerate}[rightmargin = \leftmargin]
                    \item Criação de uma sub-árvore com os símbolos de \textbf{menor} probabilidade, associando-a o somatório de suas possibilidades;
                    \item Seleção de dois símbolos ou sub-árvores com menores probabilidades e as combine em uma nova sub-árvore;
                    \begin{enumerate}[noitemsep, rightmargin = \leftmargin]
                        \item Caso hajam símbolos ou sub-árvores com mesma probabilidade, escolha arbitrariamente;
                    \end{enumerate}
                \end{enumerate}
            Consequência deste algoritmo:
                \begin{itemize}[rightmargin = \leftmargin]
                    \item Todas as codificações apresentam o mesmo comprimento esperado, logo a mesma eficiência, independente dos rótulos empregados para cada ramificação;
                    \item Desempenhos mais próximos da entropia podem ser obtidos com sequências maiores, normalmente aplicadas em algoritmos de compressão como \texttt{LZW};
                \end{itemize}

\tikzset{
    iv/.style={
        draw,
        fill=white,
        circle,
        minimum size=20pt,
        inner sep=0pt,
        text=black
    },
    ev/.style={
        draw,
        fill=gray!10,
        rectangle,
        minimum size=20pt,
        inner sep=0pt,
        text=black
    }
}
    \begin{multicols}{2}
        \begin{figure}[H]
            \centering
            \begin{forest} % from https://tex.stackexchange.com/a/304002/121799
                for tree={
                    s sep = 10mm,   % Horizontal Distance
                    l = 0mm,        % Vertical Distance
                    where n children={0}{ev}{iv},
                    l+=8mm,
                    if n=1{
                        edge label={
                            node [midway, left] {0}
                        }
                    }{
                        edge label={
                            node [midway, right] {1}
                        }
                    }
                }
                [100
                    [60
                        [A] [B]
                    ]
                    [40
                        [C]
                        [20
                            [D] [E]
                        ]
                    ]
                ]
            \end{forest}
            \caption{Representação da Árvore de Huffman}
        \end{figure} \noindent

        \columnbreak\noindent
        Considera-se como exemplo a seguinte distribuição de probabilidades:
            \begin{table}[H]
                \centering
                \begin{tabular}[]{cc}\hline
                    Símbolos & Probabilidade\\\hline
                    A & 30\\
                    B & 30\\
                    C & 20\\
                    D & 10\\
                    E & 10\\\hline
                \end{tabular}
                \caption{Probabilidades dos Símbolos}\label{table:Huffman}
            \end{table}
    
    \end{multicols}
\end{document}